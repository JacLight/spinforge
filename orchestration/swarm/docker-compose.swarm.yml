# SpinForge Docker Swarm Multi-Node Configuration
# Copyright (c) 2025 Jacob Ajiboye
# Licensed under the MIT License

version: '3.8'

services:
  keydb:
    image: eqalpha/keydb:latest
    deploy:
      replicas: 3
      placement:
        constraints:
          - node.role == manager
        max_replicas_per_node: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
    volumes:
      - keydb-data:/data
    environment:
      - KEYDB_PASSWORD=${REDIS_PASSWORD:-}
    command: keydb-server --appendonly yes --protected-mode no --port 16378 --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000
    networks:
      - spinforge-swarm
    healthcheck:
      test: ["CMD", "keydb-cli", "-p", "16378", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  spinforge-api:
    image: jaclight/spinforge-api:latest
    deploy:
      replicas: 3
      placement:
        max_replicas_per_node: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    ports:
      - "8080:8080"
    environment:
      - PORT=8080
      - ORCHESTRATION_MODE=swarm
      - KEYDB_HOST=keydb
      - KEYDB_PORT=16378
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
      - JWT_SECRET=${JWT_SECRET}
      - NODE_ENV=production
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - static-data:/data/static
      - certs-data:/data/certs
    networks:
      - spinforge-swarm
    depends_on:
      - keydb
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  spinforge-openresty:
    image: jaclight/spinforge-openresty:latest
    deploy:
      replicas: 3
      placement:
        max_replicas_per_node: 1
      restart_policy:
        condition: on-failure
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '1.0'
    ports:
      - "80:80"
      - "443:443"
    environment:
      - KEYDB_HOST=keydb
      - KEYDB_PORT=16378
    volumes:
      - ./hosting/openresty/nginx.conf:/usr/local/openresty/nginx/conf/nginx.conf:ro
      - ./hosting/openresty/lua:/etc/openresty/lua:ro
      - static-data:/data/static:ro
      - certs-data:/etc/letsencrypt:ro
      - certbot-webroot:/var/www/certbot:ro
    networks:
      - spinforge-swarm
    depends_on:
      - keydb
      - spinforge-api

  orchestrator:
    image: jaclight/spinforge-orchestrator:latest
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
    environment:
      - ORCHESTRATION_TYPE=swarm
      - KEYDB_HOST=keydb
      - KEYDB_PORT=16378
      - SCALING_ENABLED=true
      - SLEEP_ENABLED=true
      - SLEEP_TIMEOUT_MINUTES=30
      - SCALE_UP_CPU_THRESHOLD=70
      - SCALE_DOWN_CPU_THRESHOLD=20
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - spinforge-swarm
    depends_on:
      - keydb
      - spinforge-api

  certbot:
    image: certbot/certbot:latest
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
    volumes:
      - certs-data:/etc/letsencrypt
      - certbot-webroot:/var/www/certbot
      - certbot-logs:/var/log/letsencrypt
    networks:
      - spinforge-swarm
    entrypoint: "/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done;'"

  admin-ui:
    image: jaclight/spinforge-admin-ui:latest
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
      update_config:
        parallelism: 1
        delay: 5s
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    ports:
      - "8083:80"
    environment:
      - VITE_API_BASE_URL=http://spinforge-api:8080
    networks:
      - spinforge-swarm
    depends_on:
      - spinforge-api

  website:
    image: jaclight/spinforge-website:latest
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    ports:
      - "3001:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://spinforge-api:8080
    networks:
      - spinforge-swarm
    depends_on:
      - spinforge-api

  # Monitoring services
  prometheus:
    image: prom/prometheus:latest
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    ports:
      - "9090:9090"
    volumes:
      - ./orchestration/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    networks:
      - spinforge-swarm
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'

  grafana:
    image: grafana/grafana:latest
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin123}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
    networks:
      - spinforge-swarm

volumes:
  keydb-data:
    driver: local
  static-data:
    driver: local
  certs-data:
    driver: local
  certbot-webroot:
    driver: local
  certbot-logs:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

networks:
  spinforge-swarm:
    driver: overlay
    attachable: true
    ipam:
      config:
        - subnet: 10.0.9.0/24
